{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGLuYN-29bGm"
   },
   "source": [
    "# Import dependencies and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyKyMKwN9Zb3",
    "outputId": "0a5c2188-a933-4aac-b227-daf9c24b2bc4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.9 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m33.9/33.9 MB\u001B[0m \u001B[31m26.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.3.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.22.4)\n",
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.7.0.72)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Installing collected packages: sounddevice, mediapipe\n",
      "Successfully installed mediapipe-0.10.0 sounddevice-0.4.6\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.27.1)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.22.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.25.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.8)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (8.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting SpeechRecognition\n",
      "  Downloading SpeechRecognition-3.10.0-py2.py3-none-any.whl (32.8 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m32.8/32.8 MB\u001B[0m \u001B[31m33.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.27.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
      "Installing collected packages: SpeechRecognition\n",
      "Successfully installed SpeechRecognition-3.10.0\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.0.post2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.0)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.56.4)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch<1.7,>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.5)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.2)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.5)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (67.7.2)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (23.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch<1.7,>=1.0->librosa) (2.27.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (3.4)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install mediapipe\n",
    "!pip install moviepy\n",
    "!pip install SpeechRecognition\n",
    "!pip install librosa\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TCXF8tkl9g0j"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import glob\n",
    "from scipy.spatial import ConvexHull\n",
    "import math\n",
    "import csv\n",
    "from os import path\n",
    "import json\n",
    "\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "import librosa\n",
    "from moviepy.editor import VideoFileClip\n",
    "import speech_recognition as sr\n",
    "import moviepy.editor as m_editor\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5hTw3L3_BiP",
    "outputId": "ccec4aa4-4118-4c72-8fca-8d76e7f8d809"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "## connect to google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tXa5ci4G_Ip_"
   },
   "outputs": [],
   "source": [
    "# video = \"/content/drive/MyDrive/interview.mp4\"\n",
    "# video_cut = \"/content/drive/MyDrive/interview-cut.mp4\"\n",
    "# video_out = \"/content/drive/MyDrive/interview-output.mp4\"\n",
    "# output_csv = \"/content/drive/MyDrive/interview.csv\"\n",
    "video = '../../data/CHR/3035post.MTS'\n",
    "video_cut = '../data/3035post-cut.mp4'\n",
    "video_out = '../output/3035post-output.mp4'\n",
    "output_csv = '../output/3035post-gesture.csv'\n",
    "output_audio_csv = '../output/3035post-acoustic.csv'"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "input_dir = \"/content/drive/MyDrive/train/\"\n",
    "output_dir = \"/content/drive/MyDrive/train-landmarks/\""
   ],
   "metadata": {
    "id": "btjEajKIggkb"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LhnDqMlzDP_N"
   },
   "outputs": [],
   "source": [
    "# Initialize MediaPipe's Holistic module\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "holistic = mp_holistic.Holistic(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Functions\n",
    "\n"
   ],
   "metadata": {
    "id": "xSR7ybng77F8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Dataframe functions\n",
    "\n",
    "# function to split values in dataframe\n",
    "def split_xyz(x):\n",
    "  try:\n",
    "    lines = x.split('\\n')\n",
    "\n",
    "    x = float(lines[0].split(':')[1].strip())\n",
    "    y = float(lines[1].split(':')[1].strip())\n",
    "    z = float(lines[2].split(':')[1].strip())\n",
    "    return x, y, z\n",
    "  except:\n",
    "    return np.nan, np.nan, np.nan"
   ],
   "metadata": {
    "id": "j2J1byDG8C0I"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## Holistic functions\n",
    "\n",
    "# Function to calculate the Euclidean distance between two points\n",
    "def euclidean_distance(p1x, p1y, p2x, p2y):\n",
    "    return np.sqrt((p1x - p2x) ** 2 + (p1y - p2y) ** 2)\n",
    "\n",
    "# Function to calculate the openness of a pose\n",
    "def pose_openness(keypoints_x, keypoints_y, image_w, image_h):\n",
    "\n",
    "    # coords = np.array([(kp.x, kp.y) for kp in keypoints])\n",
    "    coords = np.array([(int(kp_x * image_w), int(kp_y * image_h)) for kp_x, kp_y in zip(keypoints_x, keypoints_y)])\n",
    "    hull = ConvexHull(coords)\n",
    "    \n",
    "    return hull.volume\n",
    "\n",
    "# Function to calculate leaning direction\n",
    "def leaning_direction(nose_z, lhip_z, rhip_z):\n",
    "    avg_hip_z = (lhip_z + rhip_z) / 2\n",
    "\n",
    "    if nose_z < avg_hip_z:\n",
    "        return \"Backward\"\n",
    "    else:\n",
    "        return \"Forward\"\n",
    "\n",
    "# Function to calculate head direction\n",
    "def head_direction(prev_nose_x, prev_nose_y, curr_nose_x, curr_nose_y, image_w, image_h):\n",
    "    curr_nose_cood = np.array([int(curr_nose_x * image_w), int(curr_nose_y * image_h)])\n",
    "    prev_nose_cood = np.array([int(prev_nose_x * image_w), int(prev_nose_y * image_h)])\n",
    "    nose_diff = curr_nose_cood - prev_nose_cood\n",
    "    horizontal = 'STILL'\n",
    "    vertical = 'STILL'\n",
    "    if nose_diff[0] > 0:\n",
    "        horizontal = \"RIGHT\"\n",
    "    elif nose_diff[0] < 0:\n",
    "        horizontal = 'LEFT'\n",
    "    \n",
    "    if nose_diff[1] > 0:\n",
    "        vertical = 'UP'\n",
    "    elif nose_diff[1] < 0:\n",
    "        vertical = 'DOWN'\n",
    "    return horizontal, vertical\n",
    "\n",
    "# Function to calculate angle between three landmarks\n",
    "def calculate_angle(l1x, l1y, l2x, l2y, l3x, l3y):\n",
    "\n",
    "    # Calculate the angle between the three points\n",
    "    angle = math.degrees(math.atan2(l3y - l2y, l3x - l2x) - math.atan2(l1y - l2y, l1x - l2x))\n",
    "    \n",
    "    # Check if the angle is less than zero.\n",
    "    if angle < 0:\n",
    "        # Add 360 to the found angle.\n",
    "        angle += 360\n",
    "  \n",
    "    return angle\n",
    "\n",
    "# Function to calculate angle between three landmarks using numpy module\n",
    "def numpy_angle(l1x,l1y,l2x,l2y,l3x,l3y, width, height):\n",
    "  a = np.array([l1x * width, l1y * height])\n",
    "  b = np.array([l2x * width, l2y * height])\n",
    "  c = np.array([l3x * width, l3y * height])\n",
    "\n",
    "  ba = a - b\n",
    "  bc = c - b\n",
    "\n",
    "  cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "  angle = np.arccos(cosine_angle)\n",
    "\n",
    "  return np.degrees(angle)\n",
    "\n",
    "# Function to calculate hand orientation\n",
    "def orientation(l0x, l0y, l9x,l9y): \n",
    "    x0 = l0x\n",
    "    y0 = l0y\n",
    "    \n",
    "    x9 = l9x\n",
    "    y9 = l9y\n",
    "    \n",
    "    if abs(x9 - x0) < 0.05:      # since tan(0) --> ∞\n",
    "        m = 1000000000\n",
    "    else:\n",
    "        m = abs((y9 - y0)/(x9 - x0))       \n",
    "        \n",
    "    if m>=0 and m<=1:\n",
    "        if x9 > x0:\n",
    "            return \"Right\"\n",
    "        else:\n",
    "            return \"Left\"\n",
    "    if m>1:\n",
    "        if y9 < y0:       # since, y decreases upwards\n",
    "            return \"Up\"\n",
    "        else:\n",
    "            return \"Down\""
   ],
   "metadata": {
    "id": "SrlVT7pO8JCZ"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialize Variables"
   ],
   "metadata": {
    "id": "3C3v20iz_7Ta"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Holistic\n",
    "prev_row = pd.Series(dtype='float64') #initially empty series\n",
    "total_movement = 0\n",
    "frame_movement = 0\n",
    "openness_value = 0\n",
    "head_horizontal = \"\"\n",
    "head_vertical = \"\"\n",
    "holistic_threshold = 0.001  # Adjust the threshold to fine-tune movement detection sensitivity\n",
    "holistic_keypoints1 = [\n",
    "    'PL-LEFT_WRIST',\n",
    "    'PL-RIGHT_WRIST',\n",
    "    'PL-LEFT_ANKLE',\n",
    "    'PL-RIGHT_ANKLE',\n",
    "]\n",
    "holistic_keypoints2 = [ \n",
    "    'PL-LEFT_SHOULDER',\n",
    "    'PL-RIGHT_SHOULDER',\n",
    "    'PL-LEFT_HIP',\n",
    "    'PL-RIGHT_HIP',\n",
    "    'PL-LEFT_WRIST',\n",
    "    'PL-RIGHT_WRIST'\n",
    "]\n",
    "\n",
    "## Arms\n",
    "left_arm_movement = 0\n",
    "right_arm_movement = 0\n",
    "\n",
    "la_counter = 0 \n",
    "la_orientation = None\n",
    "la_leaning = None\n",
    "\n",
    "ra_counter = 0 \n",
    "ra_orientation = None\n",
    "ra_leaning = None\n",
    "\n",
    "## Hand\n",
    "lh_tip_distance = 0\n",
    "lh_state = None\n",
    "lh_orientation = None\n",
    "\n",
    "rh_tip_distance = 0\n",
    "rh_state = None\n",
    "rh_orientation = None\n",
    "\n",
    "# moving average window\n",
    "window_size = 10"
   ],
   "metadata": {
    "id": "XVhpH5Vi__3g"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "split_columns = ['PL-LEFT_WRIST', 'PL-RIGHT_WRIST',\n",
    "       'PL-LEFT_ELBOW', 'PL-RIGHT_ELBOW', 'PL-LEFT_SHOULDER',\n",
    "       'PL-RIGHT_SHOULDER', 'PL-LEFT_ANKLE', 'PL-RIGHT_ANKLE', 'PL-LEFT_HIP',\n",
    "       'PL-RIGHT_HIP', 'PL-NOSE', 'LH-INDEX_FINGER_TIP', 'RH-INDEX_FINGER_TIP',\n",
    "       'LH-THUMB_TIP', 'RH-THUMB_TIP', 'LH-MIDDLE_FINGER_TIP',\n",
    "       'RH-MIDDLE_FINGER_TIP', 'LH-MIDDLE_FINGER_MCP', 'RH-MIDDLE_FINGER_MCP',\n",
    "       'LH-RING_FINGER_TIP', 'RH-RING_FINGER_TIP', 'LH-RING_FINGER_MCP',\n",
    "       'RH-RING_FINGER_MCP', 'LH-PINKY_TIP', 'RH-PINKY_TIP','LH-PINKY_MCP', 'RH-PINKY_MCP', 'LH-WRIST',\n",
    "       'RH-WRIST']\n",
    "\n"
   ],
   "metadata": {
    "id": "g5m8pyykM3hs"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "csv_header = [  'frame',\n",
    "                'total_movement_per_second',\n",
    "                'pose_openness',\n",
    "                'leaning',\n",
    "                'head_horizontal',\n",
    "                'head_vertical',\n",
    "                'left_arm_angle', \n",
    "                'left_arm_v_movement', \n",
    "                'left_arm_h_movement',\n",
    "                'right_arm_angle', \n",
    "                'right_arm_v_movement', \n",
    "                'right_arm_h_movement', \n",
    "                'left_hand_orientation', \n",
    "                'left_hand_state', \n",
    "                'right_hand_orientation', \n",
    "                'right_hand_state']"
   ],
   "metadata": {
    "id": "y3DIHjwvM6z-"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load landmarks CSV and extract Gesture data"
   ],
   "metadata": {
    "id": "Dx0VR0zFg7Lh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pattern = output_dir+\"*-landmarks.csv\"\n",
    "files = [path.basename(x) for x in glob.glob(pattern)]\n",
    "print(files)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yAi9rvKrw7Xb",
    "outputId": "bae6ab73-f112-426a-b99d-2b1143e5ef6d"
   },
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['couch-landmarks.csv', 'interview-landmarks.csv']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for file in files:\n",
    "  # define output csv file\n",
    "  filename = file.split('.')[0].split('-')[0]\n",
    "  features_csv = output_dir+filename+'-features.csv'\n",
    "\n",
    "  # load dataframe\n",
    "  df = pd.read_csv(output_dir+file)\n",
    "  ###df = df.head(10) ###\n",
    "\n",
    "  # split columns to x y z \n",
    "  for column in split_columns:\n",
    "    df[[column+'-X', column+'-Y', column+'-Z']] = df[column].apply(lambda x: pd.Series(split_xyz(x)))\n",
    "  \n",
    "  # drop original columns\n",
    "  df.drop(columns=split_columns, inplace=True)\n",
    "\n",
    "  # handle missing values by interpolation\n",
    "  df.interpolate(method ='linear', limit_direction ='both', inplace=True, limit=10)\n",
    "\n",
    "  # split the dataframes\n",
    "  df2 = df.loc[:, ~df.columns.isin([\"width\", \"height\", \"fps\", \"frame\"])]\n",
    "  df1 = df.loc[:, df.columns.isin([\"width\", \"height\", \"fps\", \"frame\"])]\n",
    "\n",
    "  # moving average calculation\n",
    "  df2 = df2.rolling(window_size, min_periods=1).mean()\n",
    "\n",
    "  # concatenate dataframes back together\n",
    "  df = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "\n",
    "  with open(features_csv, mode='w', newline='') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(csv_header)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "\n",
    "      ## Holistic movements\n",
    "      # Calculate the total movement\n",
    "      if not prev_row.empty:\n",
    "        frame_movement = 0\n",
    "        for kp1 in holistic_keypoints1:\n",
    "          distance = euclidean_distance(row[kp1+'-X'], row[kp1+'-Y'], prev_row[kp1+'-X'], prev_row[kp1+'-Y'])\n",
    "          frame_movement += distance\n",
    "        if frame_movement > holistic_threshold:\n",
    "          total_movement += frame_movement\n",
    "        \n",
    "        # calculate head direction\n",
    "        head_horizontal, head_vertical = head_direction(prev_row['PL-NOSE-X'], prev_row['PL-NOSE-Y'], row['PL-NOSE-X'], row['PL-NOSE-Y'], row['width'], row['height'])\n",
    "      \n",
    "      # Calculate and display the total movement and pose openness on the frame\n",
    "      kp2_x = [ row[kp2+'-X'] for kp2 in holistic_keypoints2]\n",
    "      kp2_y = [ row[kp2+'-Y'] for kp2 in holistic_keypoints2]\n",
    "      openness_value = pose_openness(kp2_x, kp2_y, row['width'], row['height'])\n",
    "\n",
    "      # Calculate and display the leaning direction\n",
    "      leaning_dir = leaning_direction(row['PL-NOSE-Z'], row['PL-LEFT_HIP-Z'], row['PL-RIGHT_HIP-Z'])\n",
    "      \n",
    "      ## Hand movements\n",
    "      # distance b/w INDEX_FINGER_TIP and THUMB_TIP\n",
    "      lh_tip_distance = euclidean_distance(row['LH-INDEX_FINGER_TIP-X'], row['LH-INDEX_FINGER_TIP-Y'], row['LH-THUMB_TIP-X'], row['LH-THUMB_TIP-Y'])\n",
    "      if row['LH-MIDDLE_FINGER_TIP-Y'] < row['LH-MIDDLE_FINGER_MCP-Y'] and row['LH-RING_FINGER_TIP-Y'] < row['LH-RING_FINGER_MCP-Y'] \\\n",
    "        and row['LH-PINKY_TIP-Y'] < row['LH-PINKY_MCP-Y'] and lh_tip_distance < 0.015:\n",
    "        lh_state = 'CLOSED'\n",
    "      else:\n",
    "        lh_state = 'OPEN'\n",
    "      lh_orientation = orientation(row['LH-WRIST-X'], row['LH-WRIST-Y'], row['LH-MIDDLE_FINGER_MCP-X'], row['LH-MIDDLE_FINGER_MCP-Y'])\n",
    "\n",
    "      rh_tip_distance = euclidean_distance(row['RH-INDEX_FINGER_TIP-X'], row['RH-INDEX_FINGER_TIP-Y'], row['RH-THUMB_TIP-X'], row['RH-THUMB_TIP-Y'])\n",
    "      if row['RH-MIDDLE_FINGER_TIP-Y'] < row['RH-MIDDLE_FINGER_MCP-Y'] and row['RH-RING_FINGER_TIP-Y'] < row['RH-RING_FINGER_MCP-Y'] \\\n",
    "        and row['RH-PINKY_TIP-Y'] < row['RH-PINKY_MCP-Y'] and rh_tip_distance < 0.015:\n",
    "        rh_state = 'CLOSED'\n",
    "      else:\n",
    "        rh_state = 'OPEN'\n",
    "      rh_orientation = orientation(row['RH-WRIST-X'], row['RH-WRIST-Y'], row['RH-MIDDLE_FINGER_MCP-X'], row['RH-MIDDLE_FINGER_MCP-Y'])\n",
    "\n",
    "\n",
    "      ## Arm movements\n",
    "      # Calculate weather arm is up or down\n",
    "      la_angle = numpy_angle(row['PL-LEFT_WRIST-X'], row['PL-LEFT_WRIST-Y'], row['PL-LEFT_ELBOW-X'], row['PL-LEFT_ELBOW-Y'], row['PL-LEFT_SHOULDER-X'], row['PL-LEFT_SHOULDER-Y'], row['width'], row['height'])\n",
    "      ra_angle = numpy_angle(row['PL-RIGHT_WRIST-X'], row['PL-RIGHT_WRIST-Y'], row['PL-RIGHT_ELBOW-X'], row['PL-RIGHT_ELBOW-Y'], row['PL-RIGHT_SHOULDER-X'], row['PL-RIGHT_SHOULDER-Y'], row['width'], row['height'])\n",
    "\n",
    "      if la_angle > 160:\n",
    "        la_orientation = \"DOWN\"\n",
    "      if la_angle < 30 and la_orientation =='DOWN':\n",
    "        la_orientation=\"UP\"\n",
    "        la_counter +=1\n",
    "\n",
    "      if ra_angle > 160:\n",
    "        ra_orientation = \"DOWN\"\n",
    "      if ra_angle < 30 and ra_orientation =='DOWN':\n",
    "        ra_orientation=\"UP\"\n",
    "        ra_counter +=1\n",
    "\n",
    "      # Calculate wheather arm is leaning forward\n",
    "      if abs(row['PL-LEFT_WRIST-Z']) > abs(row['PL-LEFT_ELBOW-Z']) :\n",
    "        la_leaning = 'FORWARD'\n",
    "      else:\n",
    "        la_leaning = 'NOT FORWARD'\n",
    "\n",
    "      if abs(row['PL-RIGHT_WRIST-Z']) > abs(row['PL-RIGHT_ELBOW-Z']):\n",
    "        ra_leaning = 'FORWARD'\n",
    "      else:\n",
    "        ra_leaning = 'NOT FORWARD'\n",
    "      \n",
    "      # write features to csv file\n",
    "      writer.writerow([row['frame'],\n",
    "                          total_movement, \n",
    "                          openness_value,\n",
    "                          leaning_dir,\n",
    "                          head_horizontal,\n",
    "                          head_vertical,\n",
    "                          la_angle, \n",
    "                          la_orientation, \n",
    "                          la_leaning, \n",
    "                          ra_angle, \n",
    "                          ra_orientation, \n",
    "                          ra_leaning, \n",
    "                          lh_orientation, \n",
    "                          lh_state, \n",
    "                          rh_orientation, \n",
    "                          rh_state\n",
    "                      ])\n",
    "      \n",
    "      # save previous row\n",
    "      prev_row = row\n"
   ],
   "metadata": {
    "id": "gLcihF1ghT5U"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jpC8RI9ntBC2"
   },
   "source": [
    "# Transcript & Acoustic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5dV_xFctNBa"
   },
   "outputs": [],
   "source": [
    "# Load the video\n",
    "clip = VideoFileClip(video_cut)\n",
    "\n",
    "# Initialize the output CSV File\n",
    "with open(output_audio_csv, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"time\", \"avg_pitch\", \"avg_intensity\", \"transcription\"])\n",
    "\n",
    "# Initialize the speech recognizer\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# Process the audio one second at a time\n",
    "for i in range(0, int(clip.duration), 10):\n",
    "    # Extract one second of audio\n",
    "    audio_segment = clip.subclip(i, i + 10).audio\n",
    "    if audio_segment is None:\n",
    "        continue\n",
    "    audio_segment.write_audiofile(\"temp_audio.wav\")\n",
    "\n",
    "    # Load the audio file with librosa\n",
    "    y, sampling_rate = librosa.load(\"temp_audio.wav\")\n",
    "\n",
    "    # Calculate pitch with librosa\n",
    "    pitches, magnitudes = librosa.piptrack(y=y, sr=sampling_rate)\n",
    "\n",
    "    # Calculate average pitch and intensity for this second\n",
    "    avg_pitch = pitches.mean()\n",
    "    avg_intensity = magnitudes.mean()\n",
    "\n",
    "    # Transcribe the audio with SpeechRecognition\n",
    "    with sr.AudioFile(\"temp_audio.wav\") as source:\n",
    "        audio = r.record(source, duration=10)  # read the entire audio file                  \n",
    "        try:\n",
    "            transcription = r.recognize_google(audio)\n",
    "        except sr.UnknownValueError:\n",
    "            transcription = \"\"\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "            transcription = \"\"\n",
    "\n",
    "    # Write the features to the CSV\n",
    "    with open(output_audio_csv, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([i, avg_pitch, avg_intensity, transcription])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51muMogHf_t4"
   },
   "source": [
    "# Cross Modality Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHtByF5ef_t4"
   },
   "outputs": [],
   "source": [
    "# First, ensure both 'time' columns are sorted in ascending order\n",
    "df_gesture = pd.read_csv('../output/3035post-gesture.csv')\n",
    "df_acoustic = pd.read_csv('../output/3035post-acoustic.csv')\n",
    "\n",
    "df_gesture = df_gesture.rename(columns={'time_in_seconds': 'time'})\n",
    "df_gesture['time'] = df_gesture['time'].astype(int)\n",
    "df_acoustic['time'] = df_acoustic['time'].astype(int)\n",
    "df_acoustic['transcription'] = df_acoustic['transcription'].astype(str)\n",
    "df_gesture = df_gesture.sort_values('time')\n",
    "df_acoustic = df_acoustic.sort_values('time')\n",
    "\n",
    "# Use merge_asof to merge\n",
    "df = pd.merge_asof(df_gesture, df_acoustic, on='time', direction='backward')\n",
    "\n",
    "# print(df)\n",
    "# Define the correspondence between words and gestures\n",
    "word_gesture_map = {\n",
    "    'up': {'head_vertical': 'up', \n",
    "           'left_arm_v_movement': 'up', \n",
    "           'right_arm_v_movement': 'up',  \n",
    "           'left_hand_orientation': 'up',\n",
    "           'right_hand_orientation': 'up'},\n",
    "    'down': {'head_vertical': 'down', \n",
    "             'left_arm_v_movement': 'down', \n",
    "             'right_arm_v_movement': 'down', \n",
    "             'left_hand_orientation': 'down', \n",
    "             'right_hand_orientation': 'down'},\n",
    "    'left': {'head_horizontal': 'left',  \n",
    "             'left_hand_orientation': 'left',\n",
    "             'right_hand_orientation': 'left'},\n",
    "    'right': {'head_horizontal': 'right', \n",
    "              'left_hand_orientation': 'right',\n",
    "              'right_hand_orientation': 'right'},\n",
    "    'open': {'left_hand_state': 'open',\n",
    "             'right_hand_state': 'open'},\n",
    "    'closed': {'left_hand_state': 'closed', \n",
    "               'right_hand_state': 'closed'},\n",
    "    'forward': {'leaning': 'forward',\n",
    "                'left_arm_h_movement': 'forward',\n",
    "                'right_arm_h_movement': 'forward'},\n",
    "    'backward': {'leaning': 'backward'}\n",
    "}\n",
    "\n",
    "def detect_contradictions(row):\n",
    "    # Extract the words from the transcript\n",
    "    words = row['transcription'].split()\n",
    "    \n",
    "    for word in words:\n",
    "        if word in word_gesture_map:\n",
    "            # Get the expected gestures for this word\n",
    "            expected_gestures = word_gesture_map[word]\n",
    "            \n",
    "            # Compare the expected gestures with the actual gestures\n",
    "            for gesture, expected_value in expected_gestures.items():\n",
    "                if row[gesture].upper() != expected_value.upper():\n",
    "                    # There is a contradiction, output the row\n",
    "                    print('contradiction')\n",
    "                    print(row[gesture], words)\n",
    "                    return row\n",
    "\n",
    "# Apply the function to each row\n",
    "contradictions = df.apply(detect_contradictions, axis=1)\n",
    "\n",
    "# Drop the rows where no contradiction was found\n",
    "contradictions.dropna(how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFAWvP4ef_t4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [
    "jpC8RI9ntBC2",
    "51muMogHf_t4"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
